<!DOCTYPE html>
<html lang="en" class="h-full antialiased">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      body {transition: opacity ease-in 0.2s; }
      body[unresolved] {opacity: 0; display: block; overflow: hidden; position: relative; }
    </style>
    <title>Can AI replace us?</title>
    <link rel="stylesheet" href="styles.css">
  </head>
  <body class="flex h-full dark:bg-black bg-zinc-50">
    <div class="flex w-full">
      <div class="flex fixed inset-0 justify-center sm:px-8">
        <div class="flex w-full max-w-7xl lg:px-8">
          <div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div>
        </div>
      </div>
      <main class="flex-auto z-1">
        <div class="mt-9 sm:px-8 pb-9">
          <div class="mx-auto w-full max-w-7xl lg:px-8">
            <div class="relative px-4 sm:px-8 lg:px-12">
              <div class="mx-auto max-w-2xl lg:max-w-5xl">
                <div class="max-w-2xl">
                  <h1 class="sm:pb-5 text-4xl font-bold tracking-tight sm:text-5xl text-zinc-800 dark:text-zinc-100">
                    No.
                  </h1>

                  <blockquote>
                    <p>The more time I spend using LLMs for code, the less I worry for my career - even as their coding capabilities continue to improve. [&hellip;]</p>
                    <p>No matter how good these things get, they will still need someone to find problems for them to solve, define those problems and confirm that they are solved. That's a job &mdash; one that other humans will be happy to outsource to an expert practitioner.</p>
                    <p>It's also about 80% of what I do as a software developer already.</p>
                  </blockquote>
                  <p>
                    Simon Willison,
                    <a href="https://simonwillison.net/2025/Jul/4/identify-solve-verify/">
                      <i>Identify, Solve, Verify</i>
                    </a>
                  </p>


                  <blockquote>
                    <p>Coding agents require the scaffolding, learning, and often demand more attention than tools, but are built to look like teammates. This makes them both unwieldy tools and lousy teammates. We should either have agents designed to look like a teammate properly act like a teammate, and barring that, have a tool that behaves like a tool. [&hellip;]
                    </p>
                    <p>
                      Selling sci-fi is way too effective. And as long as the AI is perceived as the engine of a new industrial revolution, decision-makers will imagine it can do so, and task people to make it so.<br/><br/>Things won’t change, because people are adaptable and want the system to succeed. We consequently take on the responsibility for making things work, through ongoing effort and by transforming ourselves in the process. Through that work, we make the technology appear closer to what it promises than what it actually delivers, which in turn reinforces the pressure to adopt it.<br/><br/>As we take charge of bridging the gap, the machine claims the praise.
                    </p>
                  </blockquote>
                  <p>
                    Fred Herbert,
                    <a href="https://ferd.ca/the-gap-through-which-we-praise-the-machine.html">
                      <i>The Gap Through Which We Praise the Machine</i>
                    </a>
                  </p>


                  <blockquote>
                    <p>
                      Radiology has embraced AI enthusiastically, and the labor force is growing nevertheless. The augmentation-not-automation effect of AI is despite the fact that AFAICT there is no identified &quot;task&quot; at which human radiologists beat AI. So maybe the &quot;jobs are bundles of tasks&quot; model in labor economics is incomplete. Paraphrasing something @MelMitchell1 pointed out to me, if you define jobs in terms of tasks maybe you&#39;re actually defining away the most nuanced and hardest-to-automate aspects of jobs, which are at the boundaries between tasks.
                    </p>
                    <p>
                      Can you break up your own job into a set of well-defined tasks such that if each of them is automated, your job as a whole can be automated? I suspect most people will say no. But when we think about <em>other people&#39;s jobs</em> that we don&#39;t understand as well as our own, the task model seems plausible because we don&#39;t appreciate all the nuances.
                    </p>
                  </blockquote>
                  <p>
                    Arvind Narayanan,
                    <a href="https://xcancel.com/random_walker/status/1935679764192256328">
                      @random_walker
                    </a>
                  </p>


                  <blockquote>
                    <p>
                      We already have a phrase for code that nobody understands: legacy code. [&hellip;]
                    </p>
                    <p>
                      If you don't understand the code, your only recourse is to ask AI to fix it for you, which is like paying off credit card debt with another credit card.
                    </p>
                  </blockquote>
                  <p>
                    Steve Krouse,
                    <a href="https://blog.val.town/vibe-code">
                      Vibe code is legacy code
                    </a>
                  </p>


                  <blockquote>
                    <p>
                      What are we actually saying here — that even Microsoft has to evaluate usage of “AI” directly, because it doesn’t affect performance enough to have an obvious impact otherwise? That the technology is so limp that even its biggest investor has to&nbsp;<em>strong-arm its own employees</em>&nbsp;into using it? That their own employees don’t&nbsp;<em>want</em>&nbsp;to use&nbsp;it?<br/><br/>Genuinely good new tools don’t tend to need&nbsp;<em>coercion</em>&nbsp;to fuel their adoption only a few years into their existence, right?
                    </p>
                  </blockquote>
                  <p>
                    eevee,
                    <a href="https://eev.ee/blog/2025/07/03/the-rise-of-whatever/">
                      The rise of Whatever
                    </a>
                  </p>


                  <blockquote>
                    <p>The problem is that the actual meat of human communication is a tiny fraction of the amount of symbols being spat out. Getting the actual ideas part of a message compressed well can get lost in the noise, and a better strategy is simply evasion. Expressing an actual idea will be more right in some cases, but expressing something which sounds like an actual idea is overwhelmingly likely to be very wrong unless you have strong confidence that it’s right. So the AIs optimize by being evasive and sycophantic rather than expressing ideas.</p>
                  </blockquote>
                  <p>
                    Bram Cohen,
                    <a href="https://bramcohen.com/p/ais-are-sycophantic-blithering-idiots">
                      AIs are Sycophantic Blithering Idiots
                    </a>
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </main>
    </div>
  </body>
</html>
